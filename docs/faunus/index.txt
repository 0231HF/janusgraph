[[hadoop]]
Hadoop
======

Titan-Hadoop works with TinkerPop 3's new hadoop-gremlin package for
general-purpose OLAP and also provides standalone MapReduce-based
utilities for index management.

[[titan-hadoop-tp3]]
Titan with TinkerPop's Hadoop-Gremlin
-------------------------------------

Here's a three step example showing some basic integrated Titan-TinkerPop functionality.

1. Manually define schema and then load the Grateful Dead graph from a TP3 Kryo-serialized binary file 
2. Run a VertexProgram to compute PageRanks, writing the derived graph to `output/^g`
3. Read the derived graph vertices and their computed rank values

[WARNING]
Titan 0.9.0's integration with TinkerPop 3.0.0 is still under active development.  The APIs and configuration snippets shown below may change as Titan 0.9.0 and TinkerPop 3.0.0 move through milestone releases and eventually their respective final releases.  The content of this chapter should be considered a tech preview rather than stable reference.

First step: defining schema and loading data.

[source,gremlin]
----
bin/gremlin.sh 

         \,,,/
         (o o)
-----oOOo-(3)-oOOo-----
plugin activated: tinkerpop.server
plugin activated: tinkerpop.utilities
plugin activated: tinkerpop.tinkergraph
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/dalaro/tinkerelius/tp3m6/titan-console-0.9.0-M1.works/lib/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/dalaro/tinkerelius/tp3m6/titan-console-0.9.0-M1.works/ext/hadoop-gremlin/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/dalaro/tinkerelius/tp3m6/titan-console-0.9.0-M1.works/ext/titan-all/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
INFO  com.tinkerpop.gremlin.hadoop.structure.HadoopGraph  - HADOOP_GREMLIN_LIBS is set to: /home/dalaro/tinkerelius/tp3m6/titan-console-0.9.0-M1.works/bin/../ext/hadoop-gremlin:/home/dalaro/tinkerelius/tp3m6/titan-console-0.9.0-M1.works/bin/../ext/titan-all
plugin activated: tinkerpop.hadoop
plugin activated: aurelius.titan
gremlin> :load data/grateful-dead-titan-schema.groovy
==>true
==>standardtitangraph[cassandrathrift:[127.0.0.1]]
==>com.thinkaurelius.titan.graphdb.database.management.ManagementSystem@6088451e
==>true
==>song
==>artist
==>true
==>songType
==>performances
==>name
==>weight
==>true
==>sungBy
==>writtenBy
==>followedBy
==>true
==>verticesByName
==>followsByWeight
==>true
==>null
==>null
gremlin> g = GraphFactory.open('conf/hadoop-load.properties')
==>hadoopgraph[kryoinputformat->kryooutputformat]
gremlin> r = g.compute().program(BulkLoaderVertexProgram.build().titan('conf/titan-cassandra.properties').create()).submit().get()
...
==>result[hadoopgraph[kryoinputformat->kryooutputformat],memory[size:0]]
gremlin>
----

[source,properties]
----
# hadoop-load.properties

# Hadoop-Gremlin settings
gremlin.graph=com.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphInputFormat=com.tinkerpop.gremlin.hadoop.structure.io.kryo.KryoInputFormat
gremlin.hadoop.graphOutputFormat=com.tinkerpop.gremlin.hadoop.structure.io.kryo.KryoOutputFormat
gremlin.hadoop.memoryOutputFormat=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat
gremlin.hadoop.inputLocation=data/grateful-dead-vertices.gio
gremlin.hadoop.outputLocation=output
gremlin.hadoop.deriveMemory=false
gremlin.hadoop.jarsInDistributedCache=true

# Giraph settings
giraph.SplitMasterWorker=false
giraph.minWorkers=1
giraph.maxWorkers=1
----

[source,gremlin]
----
// grateful-dead-titan-schema.groovy

// Open Titan and its ManagementSystem
titanGraph = TitanFactory.open('conf/titan-cassandra.properties')
schema = titanGraph.openManagement()
// Vertex Labels
schema.makeVertexLabel("song").make()
schema.makeVertexLabel("artist").make()
// Property Keys
schema.makePropertyKey("songType").dataType(String.class).make()
schema.makePropertyKey("performances").dataType(Integer.class).make()
nameKey = schema.makePropertyKey("name").dataType(String.class).make()
weightKey = schema.makePropertyKey("weight").dataType(Integer.class).make()
// Edge Labels
schema.makeEdgeLabel("sungBy").make()
schema.makeEdgeLabel("writtenBy").make()
followedLabel = schema.makeEdgeLabel("followedBy").make()
// Indices
schema.buildIndex("verticesByName", Vertex.class).addKey(nameKey).unique().buildCompositeIndex()
schema.buildEdgeIndex(followedLabel, "followsByWeight", Direction.BOTH, Order.decr, weightKey)
// Commit schemata and release resources
schema.commit()
titanGraph.close()
----

Second step: running PageRank.

[source,gremlin]
----
gremlin> g = GraphFactory.open('conf/run-pagerank.properties')
==>hadoopgraph[cassandrainputformat->kryooutputformat]
gremlin> r = g.compute().program(PageRankVertexProgram.build().create()).submit().get()
INFO  com.tinkerpop.gremlin.hadoop.process.computer.giraph.GiraphGraphComputer  - HadoopGremlin(Giraph): PageRankVertexProgram[alpha=0.85,iterations=30]
...
==>result[hadoopgraph[cassandrainputformat->kryooutputformat],memory[size:0]]
gremlin>
----

[source,properties]
----
# run-pagerank.properties

# Hadoop-Gremlin settings
gremlin.graph=com.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphInputFormat=com.thinkaurelius.titan.hadoop.formats.cassandra.CassandraInputFormat
gremlin.hadoop.graphOutputFormat=com.tinkerpop.gremlin.hadoop.structure.io.kryo.KryoOutputFormat
gremlin.hadoop.memoryOutputFormat=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat
gremlin.hadoop.inputLocation=.
gremlin.hadoop.outputLocation=output
gremlin.hadoop.deriveMemory=true
gremlin.hadoop.jarsInDistributedCache=true

input.conf.storage.backend=cassandra
cassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner

# Giraph settings
giraph.SplitMasterWorker=false
giraph.minWorkers=1
giraph.maxWorkers=1
----

Third step: reading and printing ranks.

[source,gremlin]
----
gremlin> g = GraphFactory.open('conf/read-pagerank-results.properties')
==>hadoopgraph[kryoinputformat->nulloutputformat]
gremlin> g.V().map{[it.get().value('name'), it.get().value(PageRankVertexProgram.PAGE_RANK)]}
==>[BIG BOSS MAN, 0.612518225466592]
==>[WEATHER REPORT SUITE, 0.7317693791428082]
==>[HELL IN A BUCKET, 1.6428823764685747]
...
==>[Medley_Russell, 0.21375000000000002]
==>[F_&_B_Bryant, 0.21375000000000002]
==>[Johnny_Otis, 0.1786280514597559]
gremlin>
----

[source,properties]
----
# read-pagerank-results.properties
# Hadoop-Gremlin settings
gremlin.graph=com.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphInputFormat=com.tinkerpop.gremlin.hadoop.structure.io.kryo.KryoInputFormat
gremlin.hadoop.graphOutputFormat=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat
gremlin.hadoop.memoryOutputFormat=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat
gremlin.hadoop.inputLocation=output/^g
gremlin.hadoop.outputLocation=output
gremlin.hadoop.deriveMemory=false
gremlin.hadoop.jarsInDistributedCache=true

# Giraph settings
giraph.SplitMasterWorker=false
giraph.minWorkers=1
giraph.maxWorkers=1
----

[[mr-index-processing]]
MapReduce Reindexing and Index Removal
--------------------------------------

[[reindex]]
MapReduce Reindexing
~~~~~~~~~~~~~~~~~~~~

<<graph-indexes>> and <<vertex-indexes>> describe how to build graph-global and vertex-centric indexes to improve query performance. These indexes are immediately available if the indexed keys or labels have been newly defined in the same management transaction. In this case, there is no need to reindex the graph and this section can be skipped. If the indexed keys or labels already existed prior to index construction it is necessary to reindex the entire graph in order to ensure that the index contains previously added elements. This section describes the reindexing process.

[WARNING]
Reindexing is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.

Overview
^^^^^^^^

Titan can begin writing incremental index updates right after an index is defined.  However, before the index is complete and usable, Titan must also take a one-time read pass over all existing graph elements associated with the newly indexed schema type(s).  Titan uses Hadoop MapReduce for this task in order to scale reindexing to very large graphs. Once this reindexing job has completed, the index is fully populated and ready to be used. The index must then be enabled to be used during query processing.

Prior to Reindex
^^^^^^^^^^^^^^^^

The starting point of the reindexing process is the construction of an index. Refer to <<indexes>> for a complete discussion of global graph and vertex-centric indexes. Note, that a global graph index is uniquely identified by its name. A vertex-centric index is uniquely identified by the combination of its name and the edge label or property key on which the index is defined - the name of the latter is referred to as the *index type* in this section and only applies to vertex-centric indexes.

After building a new index against existing schema elements it is recommended to wait a few minutes for the index to be announced to the cluster. Note the index name (and the index type in case of a vertex-centric index) since this information is needed when reindexing.

Preparing to Reindex
^^^^^^^^^^^^^^^^^^^^

Reindexing large, horizontally-distributed databases generally proceeds most quickly on a Hadoop MapReduce cluster.  However, it's also possible to reindex from a single machine using Hadoop's "local" job runner.  Either method will work from a correctness standpoint.  The only differentiator is reindex throughput and setup complexity.

Reindexing requires:

* The index name (a string -- the user provides this to Titan when building a new index)
* The index type (a string -- the name of the edge label or property key on which the vertex-centric index is built). This applies only to vertex-centric indexes - leave blank for global graph indexes.
* An TitanGraph configuration file (a file whose path can be passed to `TitanFactory.open`)

Executing a Reindex Job
^^^^^^^^^^^^^^^^^^^^^^^

The recommended way to generate and run a reindex job is through the `MapReduceIndexJobs` helper class.  It has a pair of helper methods to generate a MapReduce configuration, configure a MR Job with a `MapReduceIndexJobsMapper`, and then run the job using the default job runner.

The two recommended entry points on the `MapReduceIndexJobs` helper class are these methods:

* `cassandraRepair`
* `hbaseRepair`

Cassandra Helper
++++++++++++++++

The static helper method `MapReduceIndexJobs.cassandraRepair` takes a path to a TitanGraph properties file (the same file that one would pass to `TitanFactory.open`), an index name, an (optional) index type, and the name of the Cassandra keyspace partitioner.  It generates a reindexing config file in memory and starts a job using the generated config.

The following starts the reindexing process for the global graph index `byName` defined in <<graph-indexes>>:

[source,gremlin]
MapReduceIndexJobs.cassandraRepair("conf/titan-cassandra-es.properties", "byName", "",
   /* Replace with your actual partitioner when not using Murmur3 */
   "org.apache.cassandra.dht.Murmur3Partitioner")

The following starts the reindexing process for the vertex-centric index `battlesByTime` build over `battled` edges as defined in <<vertex-indexes>>:

[source,gremlin]
MapReduceIndexJobs.cassandraRepair("conf/titan-cassandra-es.properties", "battlesByTime", "battled",
   /* Replace with your actual partitioner when not using Murmur3 */
   "org.apache.cassandra.dht.Murmur3Partitioner")


HBase Helper
++++++++++++

The static helper method `MapReduceIndexJobs.hbaseRepair` takes a path to a TitanGraph properties file (the same file that one would pass to `TitanFactory.open`), an index name, and an (optional) index type.  Unlike its Cassandra counterpart, there is no need to pass a partitioner name to the HBase helper method.  It generates a reindexing config file in memory and starts a job using the generated config.

The following starts the reindexing process for the global graph index `byName` defined in <<graph-indexes>>:

[source,gremlin]
MapReduceIndexJobs.hbaseRepair("conf/titan-hbase-es.properties", "byName", "")

The following starts the reindexing process for the vertex-centric index `battlesByTime` build over `battled` edges as defined in <<vertex-indexes>>:

[source,gremlin]
MapReduceIndexJobs.hbaseRepair("conf/titan-hbase-es.properties", "battlesByTime", "battled")

Enabling Index
^^^^^^^^^^^^^^

When the reindex job completes successfully, the index is fully populated and ready to be used. To alert all Titan instances that the index can be used for query answering, the index must be enabled in the management system.

[source,gremlin]
mgmt = g.openManagement()
rindex = mgmt.getRelationIndex(mgmt.getRelationType("battled"),"battlesByTime")
mgmt.updateIndex(rindex, SchemaAction.ENABLE_INDEX);
gindex = mgmt.getGraphIndex("byName")
mgmt.updateIndex(gindex, SchemaAction.ENABLE_INDEX);
mgmt.commit()


Self-contained Example
^^^^^^^^^^^^^^^^^^^^^^

The following Gremlin snippet outlines all steps of the reindex process in one self-contained example using minimal dummy data against the Cassandra storage backend.

[source,gremlin]
----
// Open a graph
g = TitanFactory.open("conf/titan-cassandra-es.properties")

// Define a property
mgmt = g.openManagement()
desc = mgmt.makePropertyKey("desc").dataType(String.class).make()
mgmt.commit()

// Insert some data
v = g.addVertex(null)
v.desc = "foo bar"
v = g.addVertex(null)
v.desc = "foo baz"
g.commit()

// Run a query -- note the planner warning recommending the use of an index
g.V().has("desc", CONTAINS, "baz").map

// Create an index
mgmt = g.openManagement()
desc = mgmt.getPropertyKey("desc")
mixedIndex = mgmt.buildIndex("mixedExample",Vertex.class).addKey(desc).buildMixedIndex("search")
mgmt.commit()

// Rollback or commit transactions on the graph which predate the index definition
g.rollback()

// Block until the SchemaStatus transitions from INSTALLED to REGISTERED
registered = false
before = System.currentTimeMillis()
while (!registered) {
    Thread.sleep(500L)
    mgmt = g.openManagement()
    idx  = mgmt.getGraphIndex("mixedExample")
    registered = true
    for (k in idx.getFieldKeys()) {
        s = idx.getIndexStatus(k)
        registered &= s.equals(SchemaStatus.REGISTERED)
    }
    mgmt.rollback()
}
println("Index REGISTERED in " + (System.currentTimeMillis() - before) + " ms")

// Run a Titan-Hadoop job to reindex (replace Murmur3 with your actual partitioner)
pt = "org.apache.cassandra.dht.Murmur3Partitioner" // The default
MapReduceIndexJobs.cassandraRepair("conf/titan-cassandra-es.properties", "mixedExample", "", pt)

// Enable the index
mgmt = g.openManagement()
mgmt.updateIndex(mgmt.getGraphIndex("mixedExample"), SchemaAction.ENABLE_INDEX);
mgmt.commit()

// Check the status -- should be ENABLED
mgmt = g.openManagement()
desc = mgmt.getPropertyKey("desc")
mgmt.getGraphIndex("mixedExample").getIndexStatus(desc)
mgmt.rollback()

// Run a query -- Titan will use the new index, no planner warning
g.V().has("desc", CONTAINS, "baz").map

// Concerned that Titan could have read cache in that last query, instead of relying on the index?
// Start a new instance to rule out cache hits.  Now we're definitely using the index.
g.close()
g = TitanFactory.open("conf/titan-cassandra-es.properties")
g.V().has("desc", CONTAINS, "baz").map
----

[[mr-index-removal]]
MapReduce Index Removal
~~~~~~~~~~~~~~~~~~~~~~~

[WARNING]
Index removal is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.

Overview
^^^^^^^^

Index removal is a two-stage process.  In the first stage, one Titan signals to all others via the storage backend that the index is slated for deletion.  This changes the index's state to `DISABLED`.  At that point, Titan stops using the index to answer queries and stops incrementally updating the index.  Index-related data in the storage backend remains present but ignored. the second stage, a background job deletes the index's data from the storage backend.

Preparing for Index Removal
^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the index is currently enabled and in use, it should first be disabled.  This is done through the `ManagementSystem`.

[source,gremlin]
mgmt = g.openManagement()
rindex = mgmt.getRelationIndex(mgmt.getRelationType("battled"),"battlesByTime")
mgmt.updateIndex(rindex, SchemaAction.DISABLE_INDEX);
gindex = mgmt.getGraphIndex("byName")
mgmt.updateIndex(gindex, SchemaAction.DISABLE_INDEX);
mgmt.commit()

Once the status of all keys on the index changes to `DISABLED`, the index is ready to be removed.

Removing an index from a large, horizontally-distributed database generally proceeds most quickly on a Hadoop MapReduce cluster.  However, it's also possible to remove an index using Hadoop's "local" job runner.  Either method will work from a correctness standpoint.  The only differentiator is remove an index throughput and setup complexity.

Index removal requires:

* The index name (a string -- the user provides this to Titan when building a new index)
* The index type (a string -- the name of the edge label or property key on which the vertex-centric index is built). This applies only to vertex-centric indexes - leave blank for global graph indexes.
* An TitanGraph configuration file (a file whose path can be passed to `TitanFactory.open`)

Executing an Index Removal  Job
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The recommended way to generate and run an index removal job is through the `MapReduceIndexJobs` helper class.  It has a pair of helper methods to generate a MapReduce configuration, configure a MR Job with a `MapReduceIndexJobsMapper`, and then run the job using the default job runner.

The two recommended entry points on the `MapReduceIndexJobs` helper class are these methods:

* `cassandraRemove`
* `hbaseRemove`

Cassandra Helper
++++++++++++++++

The static helper method `MapReduceIndexJobs.cassandraRemove` takes a path to a TitanGraph properties file (the same file that one would pass to `TitanFactory.open`), an index name, an (optional) index type, and the name of the Cassandra keyspace partitioner.  It generates an index removal config file in memory and starts a job using the generated config.

The following starts an index removal job for the global graph index `byName` defined in <<graph-indexes>>:

[source,gremlin]
MapReduceIndexJobs.cassandraRemove("conf/titan-cassandra-es.properties", "byName", "",
   /* Replace with your actual partitioner when not using Murmur3 */
   "org.apache.cassandra.dht.Murmur3Partitioner")

The following starts an index removal job for the vertex-centric index `battlesByTime` build over `battled` edges as defined in <<vertex-indexes>>:

[source,gremlin]
MapReduceIndexJobs.cassandraRemove("conf/titan-cassandra-es.properties", "battlesByTime", "battled",
   /* Replace with your actual partitioner when not using Murmur3 */
   "org.apache.cassandra.dht.Murmur3Partitioner")


HBase Helper
++++++++++++

The static helper method `MapReduceIndexJobs.hbaseRemove` takes a path to a TitanGraph properties file (the same file that one would pass to `TitanFactory.open`), an index name, and an (optional) index type.  Unlike its Cassandra counterpart, there is no need to pass a partitioner name to the HBase helper method.  It generates a reindexing config file in memory and starts a job using the generated config.

The following starts the reindexing process for the global graph index `byName` defined in <<graph-indexes>>:

[source,gremlin]
MapReduceIndexJobs.hbaseRemove("conf/titan-hbase-es.properties", "byName", "")

The following starts the reindexing process for the vertex-centric index `battlesByTime` build over `battled` edges as defined in <<vertex-indexes>>:

[source,gremlin]
MapReduceIndexJobs.hbaseRemove("conf/titan-hbase-es.properties", "battlesByTime", "battled")

Common Problems with Index Jobs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

IllegalArgumentException when starting job
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When a reindexing job is started shortly after a the index has been built, the job might fail with an exception like one of the following:

[source,txt]
The index mixedExample is in an invalid state and cannot be indexed.
The following index keys have invalid status: desc has status INSTALLED
(status must be one of [REGISTERED, ENABLED])

[source,txt]
The index mixedExample is in an invalid state and cannot be indexed.
The index has status INSTALLED, but one of [REGISTERED, ENABLED] is required

When an index is built, its existence is broadcast to all other Titan instances in the cluster. Those must acknowledge the existence of the index before the reindexing process can be started. The acknowledgements can take a while to come in depending on the size of the cluster and the connection speed. Hence, one should wait a few minutes after building the index and before starting the reindex process.

Note, that the acknowledgement might fail due to Titan instance failure. In other words, the cluster might wait indefinitely on the acknowledgement of a failed instance. In this case, the user must manually remove the failed instance from the cluster registry as described in <<failure-recovery>>. After the cluster state has been restored, the acknowledgement process must be reinitiated by manually registering the index again in the management system.

[source,gremlin]
mgmt = g.openManagement()
rindex = mgmt.getRelationIndex(mgmt.getRelationType("battled"),"battlesByTime")
mgmt.updateIndex(rindex, SchemaAction.REGISTER_INDEX);
gindex = mgmt.getGraphIndex("byName")
mgmt.updateIndex(gindex, SchemaAction.REGISTER_INDEX);
mgmt.commit()

After waiting a few minutes for the acknowledgement to arrive the reindex job should start successfully.

Could not find index
^^^^^^^^^^^^^^^^^^^^

This exception in the reindexing job indicates that an index with the given name does not exist or that the name has not been specified correctly. When reindexing a global graph index, only the name of the index as defined when building the index should be specified. When reindexing a global graph index, the name of the index must be given in addition to the name of the edge label or property key on which the vertex-centric index is defined.

Cassandra Mappers Fail with "Too many open files"
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The end of the exception stacktrace may look like this:

----
java.net.SocketException: Too many open files
        at java.net.Socket.createImpl(Socket.java:447)
        at java.net.Socket.getImpl(Socket.java:510)
        at java.net.Socket.setSoLinger(Socket.java:988)
        at org.apache.thrift.transport.TSocket.initSocket(TSocket.java:118)
        at org.apache.thrift.transport.TSocket.<init>(TSocket.java:109)
----

When running Cassandra with virtual nodes enabled, the number of virtual nodes seems to set a floor under the number of mappers.  Cassandra may generate more mappers than virtual nodes for clusters with lots of data, but it seems to generate at least as many mappers as there are virtual nodes even though the cluster might be empty or close to empty.  The default is 256 as of this writing.

Each mapper opens and quickly closes several sockets to Cassandra.  The kernel on the client side of those closed sockets goes into asynchronous TIME_WAIT, since Thrift uses SO_LINGER.  Only a small number of sockets are open at any one time -- usually low single digits -- but potentially many lingering sockets can accumulate in TIME_WAIT.  This accumulation is most pronounced when running a reindex job locally (not on a distributed MapReduce cluster), since all of those client-side TIME_WAIT sockets are lingering on a single client machine instead of being spread out across many machines in a cluster.   Combined with the floor of 256 mappers, a reindex job can open thousands of sockets of the course of its execution.  When these sockets all linger in TIME_WAIT on the same client, they have the potential to reach the open-files ulimit, which also controls the number of open sockets.  The open-files ulimit is often set to 1024.

Here are a few suggestions for dealing with the "Too many open files" problem during reindexing on a single machine:

* Reduce the maximum size of the Cassandra connection pool.  For example, consider setting the cassandrathrift storage backend's `max-active` and `max-idle` options to 1 each, and setting `max-total` to -1.  See <<titan-config-ref>> for full listings of connection pool settings on the Cassandra storage backends.
* Increase the `nofile` ulimit.  The ideal value depends on the size of the Cassandra dataset and the throughput of the reindex mappers; if starting at 1024, try an order of magnitude larger: 10000.  This is just necessary to sustain lingering TIME_WAIT sockets.  The reindex job won't try to open nearly that many sockets at once.
* Run the reindex task on a multi-node MapReduce cluster to spread out the socket load.
